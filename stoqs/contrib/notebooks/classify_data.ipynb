{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Data ###\n",
    "\n",
    "*Create a classifier for different kinds of plankton using supervised machine learning* \n",
    "\n",
    "Executing this Notebook requires a personal STOQS database. Follow the [steps to build your own development system](https://github.com/stoqs/stoqs/blob/master/README.md) &mdash; this will take a few hours and depends on a good connection to the Internet.  Once your server is up log into it (after a `cd ~/Vagrants/stoqsvm`) and activate your virtual environment with the usual commands:\n",
    "\n",
    "    vagrant ssh -- -X\n",
    "    cd ~/dev/stoqsgit\n",
    "    source venv-stoqs/bin/activate\n",
    "    \n",
    "Then load the `stoqs_september2013` database with the commands:\n",
    "\n",
    "    cd stoqs\n",
    "    ln -s mbari_campaigns.py campaigns.py\n",
    "    export DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5432/stoqs\n",
    "    loaders/load.py --db stoqs_september2013\n",
    "    loaders/load.py --db stoqs_september2013 --updateprovenance\n",
    "   \n",
    "Loading this database can take over a day as there are over 40 million measurments from 22 different platforms. You may want to edit the `stoqs/loaders/CANON/loadCANON_september2013.py` file and comment all but the `loadDorado()` method calls at the end of the file. You can also set a stride value or use the `--test` option to create a `stoqs_september2013_t` database, in which case you'll need to set the STOQS_CAMPAIGNS envrironment variable: \n",
    "\n",
    "    export STOQS_CAMPAIGNS=stoqs_september2013_t\n",
    "\n",
    "Use the `stoqs/contrib/analysis/classify.py` script to create some labeled data that we will learn from:\n",
    "\n",
    "    contrib/analysis/classify.py --createLabels --groupName Plankton \\\n",
    "        --database stoqs_september2013 --platform dorado \\\n",
    "        --start 20130916T124035 --end 20130919T233905 \\\n",
    "        --inputs bbp700 fl700_uncorr --discriminator salinity \\\n",
    "        --labels diatom dino1 dino2 sediment \\\n",
    "        --mins 33.33 33.65 33.70 33.75 --maxes 33.65 33.70 33.75 33.93 --clobber -v\n",
    "\n",
    "A little explanation is probably warranted here. The Dorado missions on 16-19 September 2013 sampled distinct water types in Monterey Bay that are easily identified by ranges of salinity. These water types contain different kinds of particles as identified by bbp700 (backscatter) and fl700_uncorr (chlorophyll). The previous command \"labeled\" MeasuredParameters in the database according to our understanding of the optical properties of diatoms, dinoflagellates, and sediment. This works for this data set because of the particular oceanographic conditions at the time.\n",
    "\n",
    "This Notebook demonstrates creating a classification algortithm from these labeled data and addresses [Issue 227 on GitHub](https://github.com/stoqs/stoqs/issues/227). To be able to execute the cells and experiment with different algortithms and parameters launch Jupyter Notebook with:\n",
    "\n",
    "    cd contrib/notebooks\n",
    "    ../../manage.py shell_plus --notebook\n",
    "    \n",
    "navigate to this file and open it. You will then be able to execute the cells and experiment with different settings and code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code from the classify module to read data from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from contrib.analysis.classify import Classifier\n",
    "c = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build up command-line parameters so that we can call methods on our Classifier() object `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "ns = Namespace()\n",
    "ns.database = 'stoqs_september2013'\n",
    "ns.classifier='Decision_Tree'\n",
    "ns.inputs=['bbp700', 'fl700_uncorr']\n",
    "ns.labels=['diatom', 'dino1', 'dino2', 'sediment']\n",
    "ns.test_size=0.4\n",
    "ns.train_size=0.4\n",
    "ns.verbose=True\n",
    "c.args = ns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load the labeled data, normalize, and and split into train and test sets (borrowing from classify.py's createClassifier() method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/dev/stoqsgit/stoqs/contrib/analysis/classify.py:219: UserWarning: count = 0 for label = diatom\n",
      "  warnings.warn('count = 0 for label = {}'.format(label))\n",
      "\n",
      "/home/vagrant/dev/stoqsgit/stoqs/contrib/analysis/classify.py:219: UserWarning: count = 0 for label = sediment\n",
      "  warnings.warn('count = 0 for label = {}'.format(label))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count = 0 for label = diatom\n",
      "count = 0 for label = sediment\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X, y = c.loadLabeledData('Labeled Plankton', classes=('diatom', 'sediment'))\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=c.args.test_size, train_size=c.args.train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pylab as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams['figure.figsize'] = (27, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot classifier comparisons as in http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-03d2add12456>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vagrant/dev/stoqsgit/venv-stoqs/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vagrant/dev/stoqsgit/venv-stoqs/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 407\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKoAAADICAYAAAB4SnrTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVEvBQbUUHFRLwUG1FP4PtaDudMhMdi4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f42618bc0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (name, clf) in enumerate(c.classifiers.iteritems()):\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n",
    "                         np.arange(y_min, y_max, .02))\n",
    "    \n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(1, len(c.classifiers) + 1, i + 1)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "               alpha=0.6)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(name)\n",
    "    ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "            size=15, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2431e487a680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadLabeledData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Labeled Plankton'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'diatom'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sediment'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mShuffleSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.learning_curve import learning_curve\n",
    "#follow example: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#example-model-\n",
    "#selection-plot-learning-curve-py\n",
    "def plot_learning_curve(clf, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        print(name)\n",
    "        plt.ylim(*ylim)\n",
    "        plt.xlabel(\"Training examples\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        '''\n",
    "\t\tclf: classifiers\n",
    "\t\t\n",
    "\t\tn_jobs : # of jobs to run in parallel\n",
    "\t\t\n",
    "        cv: # of folds\n",
    "\t\t\n",
    "\t\tylim: minimum and maximum value of the y axis\n",
    "\t\t\n",
    "        '''\n",
    "        train_sizes, train_scores, test_scores = learning_curve(clf, X, y, cv=cv,\n",
    "                                                                n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        plt.grid()\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "X, y = c.loadLabeledData('Labeled Plankton', classes=('diatom', 'sediment'))\n",
    "X = StandardScaler().fit_transform(X)\n",
    "cv=cross_validation.ShuffleSplit(X.shape[0], n_iter=10, test_size=c.args.test_size, random_state=0)\n",
    "for i, (name, clf) in enumerate(c.classifiers.iteritems()):\n",
    "    plot_learning_curve(clf, name, X, y, ylim=(0, 1.01), cv=cv, n_jobs=1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
